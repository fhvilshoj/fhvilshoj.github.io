---
title: "Attention Mechanisms for Explanations"
type: "Ongoing"
permalink: /projects/2019-fall-attention-mechanisms-for-explanations
date: 2019-11-01
tags:
  - Attentions
  - Explainability
---
Many local post-hoc explainability techniques, such as DeConvNet, Guided Backprop, Layer-wise relevance propagation, and integrated gradients, rely 
on "gradient-like" computations, where explanations are propagated backwards through Neural Networks, one layer at a time.
One can alter this backward computation to include attentions, which guides the explanation techniques to produce better explanations.




